{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate sdvlm\n",
    "# pip install jupyterlab\n",
    "# jupyter lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3ac78c-c030-4f08-b2f2-0f6a821a4ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/than/miniconda3/envs/sdvlm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "xFormers not available\n",
      "xFormers not available\n",
      "/home/than/miniconda3/envs/sdvlm/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|â–ˆ| 4/4 [00:03<00:00,  1.11it/s\n",
      "Some weights of the model checkpoint at cpystan/SD-VLM-7B were not used when initializing LlavaLlamaForCausalLM: ['depth.pretrained.blocks.0.ls1.weight', 'depth.pretrained.blocks.0.ls2.weight', 'depth.pretrained.blocks.1.ls1.weight', 'depth.pretrained.blocks.1.ls2.weight', 'depth.pretrained.blocks.10.ls1.weight', 'depth.pretrained.blocks.10.ls2.weight', 'depth.pretrained.blocks.11.ls1.weight', 'depth.pretrained.blocks.11.ls2.weight', 'depth.pretrained.blocks.12.ls1.weight', 'depth.pretrained.blocks.12.ls2.weight', 'depth.pretrained.blocks.13.ls1.weight', 'depth.pretrained.blocks.13.ls2.weight', 'depth.pretrained.blocks.14.ls1.weight', 'depth.pretrained.blocks.14.ls2.weight', 'depth.pretrained.blocks.15.ls1.weight', 'depth.pretrained.blocks.15.ls2.weight', 'depth.pretrained.blocks.16.ls1.weight', 'depth.pretrained.blocks.16.ls2.weight', 'depth.pretrained.blocks.17.ls1.weight', 'depth.pretrained.blocks.17.ls2.weight', 'depth.pretrained.blocks.18.ls1.weight', 'depth.pretrained.blocks.18.ls2.weight', 'depth.pretrained.blocks.19.ls1.weight', 'depth.pretrained.blocks.19.ls2.weight', 'depth.pretrained.blocks.2.ls1.weight', 'depth.pretrained.blocks.2.ls2.weight', 'depth.pretrained.blocks.20.ls1.weight', 'depth.pretrained.blocks.20.ls2.weight', 'depth.pretrained.blocks.21.ls1.weight', 'depth.pretrained.blocks.21.ls2.weight', 'depth.pretrained.blocks.22.ls1.weight', 'depth.pretrained.blocks.22.ls2.weight', 'depth.pretrained.blocks.23.ls1.weight', 'depth.pretrained.blocks.23.ls2.weight', 'depth.pretrained.blocks.3.ls1.weight', 'depth.pretrained.blocks.3.ls2.weight', 'depth.pretrained.blocks.4.ls1.weight', 'depth.pretrained.blocks.4.ls2.weight', 'depth.pretrained.blocks.5.ls1.weight', 'depth.pretrained.blocks.5.ls2.weight', 'depth.pretrained.blocks.6.ls1.weight', 'depth.pretrained.blocks.6.ls2.weight', 'depth.pretrained.blocks.7.ls1.weight', 'depth.pretrained.blocks.7.ls2.weight', 'depth.pretrained.blocks.8.ls1.weight', 'depth.pretrained.blocks.8.ls2.weight', 'depth.pretrained.blocks.9.ls1.weight', 'depth.pretrained.blocks.9.ls2.weight', 'depth_projector.0.bias', 'depth_projector.0.weight', 'depth_projector.1.bias', 'depth_projector.1.weight', 'depth_projector.3.bias', 'depth_projector.3.weight', 'depth_projector.4.bias', 'depth_projector.4.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight']\n",
      "- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlavaLlamaForCausalLM were not initialized from the model checkpoint at cpystan/SD-VLM-7B and are newly initialized: ['depth.pretrained.blocks.0.ls1.gamma', 'depth.pretrained.blocks.0.ls2.gamma', 'depth.pretrained.blocks.1.ls1.gamma', 'depth.pretrained.blocks.1.ls2.gamma', 'depth.pretrained.blocks.10.ls1.gamma', 'depth.pretrained.blocks.10.ls2.gamma', 'depth.pretrained.blocks.11.ls1.gamma', 'depth.pretrained.blocks.11.ls2.gamma', 'depth.pretrained.blocks.12.ls1.gamma', 'depth.pretrained.blocks.12.ls2.gamma', 'depth.pretrained.blocks.13.ls1.gamma', 'depth.pretrained.blocks.13.ls2.gamma', 'depth.pretrained.blocks.14.ls1.gamma', 'depth.pretrained.blocks.14.ls2.gamma', 'depth.pretrained.blocks.15.ls1.gamma', 'depth.pretrained.blocks.15.ls2.gamma', 'depth.pretrained.blocks.16.ls1.gamma', 'depth.pretrained.blocks.16.ls2.gamma', 'depth.pretrained.blocks.17.ls1.gamma', 'depth.pretrained.blocks.17.ls2.gamma', 'depth.pretrained.blocks.18.ls1.gamma', 'depth.pretrained.blocks.18.ls2.gamma', 'depth.pretrained.blocks.19.ls1.gamma', 'depth.pretrained.blocks.19.ls2.gamma', 'depth.pretrained.blocks.2.ls1.gamma', 'depth.pretrained.blocks.2.ls2.gamma', 'depth.pretrained.blocks.20.ls1.gamma', 'depth.pretrained.blocks.20.ls2.gamma', 'depth.pretrained.blocks.21.ls1.gamma', 'depth.pretrained.blocks.21.ls2.gamma', 'depth.pretrained.blocks.22.ls1.gamma', 'depth.pretrained.blocks.22.ls2.gamma', 'depth.pretrained.blocks.23.ls1.gamma', 'depth.pretrained.blocks.23.ls2.gamma', 'depth.pretrained.blocks.3.ls1.gamma', 'depth.pretrained.blocks.3.ls2.gamma', 'depth.pretrained.blocks.4.ls1.gamma', 'depth.pretrained.blocks.4.ls2.gamma', 'depth.pretrained.blocks.5.ls1.gamma', 'depth.pretrained.blocks.5.ls2.gamma', 'depth.pretrained.blocks.6.ls1.gamma', 'depth.pretrained.blocks.6.ls2.gamma', 'depth.pretrained.blocks.7.ls1.gamma', 'depth.pretrained.blocks.7.ls2.gamma', 'depth.pretrained.blocks.8.ls1.gamma', 'depth.pretrained.blocks.8.ls2.gamma', 'depth.pretrained.blocks.9.ls1.gamma', 'depth.pretrained.blocks.9.ls2.gamma', 'depth_projector.class_embedding', 'depth_projector.patch_embedding.weight', 'depth_projector.position_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import get_model_name_from_path\n",
    "from llava.mm_utils import tokenizer_image_token, process_images\n",
    "from llava.constants import IMAGE_TOKEN_INDEX\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "model_path = \"cpystan/SD-VLM-7B\"\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_name=get_model_name_from_path(model_path),\n",
    "    model_base=None,\n",
    "    load_8bit=True,\n",
    "    load_4bit=False,\n",
    "    mm_vision_tower=\"openai/clip-vit-large-patch14-336\"\n",
    ")\n",
    "model.gt_depth = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90af19a-7172-431e-a788-3801d177979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"What is the width of this table?\"\n",
    "prompt = \"What are the dimenions of this table in meters?\"\n",
    "image_file = \"example_data/table.jpg\"\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "image = Image.open(image_file).convert('RGB')\n",
    "image_tensor = process_images([image], image_processor, model.config)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d124987-4ebd-499d-8b20-6b5173cb7a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the table are 1.41 m x 0.72 m x 0.77 m.\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.2\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor.unsqueeze(0).half().cuda(),\n",
    "        image_sizes=[image.size],\n",
    "        do_sample=True if temperature > 0 else False,\n",
    "        temperature=temperature,\n",
    "        top_p=None,\n",
    "        num_beams=1,\n",
    "        ori_imgs = [image],\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True,)\n",
    "\n",
    "response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b6d0eb-e98f-46eb-a690-67a259d64b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "134e8fd7-86af-4c7c-ad6d-df7679918279",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"cpystan/MSMU\"\n",
    "data = load_dataset(data_path, streaming=False)['test']\n",
    "#data = load_dataset(data_path, streaming=False)['train']  # Might need to stream -- its big!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d66ae-440f-437f-9380-4edce5b89984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "RESPONSE The width  of the wall shelves is 0.4 meters.\n",
      "GNDTRUTH the wall shelves measures 0.37 meters in width.\n",
      "1\n",
      "RESPONSE The width  of the cluttered shelf is 0.45 meters.\n",
      "GNDTRUTH the cluttered shelf measures 54.91 cm in width.\n",
      "2\n",
      "RESPONSE the cluttered shelf is more closer.\n",
      "GNDTRUTH the office chair is more closer.\n",
      "3\n",
      "RESPONSE The distance is 0.86 m.\n",
      "GNDTRUTH the trash can and the wooden bookshelf are 0.7 m apart from each other.\n",
      "4\n",
      "RESPONSE The height of the black chair is 0.81 m and the wooden bookshelf is 0.8 meters, so the black chair is taller.\n",
      "GNDTRUTH The height of the black chair is 78.14 cm and the wooden bookshelf is 118.68 cm, so the wooden bookshelf is taller.\n",
      "5\n",
      "RESPONSE The height of the wooden bookshelf is 1.03 meters.\n",
      "GNDTRUTH The height of the wooden bookshelf is 1.19 meters.\n",
      "6\n",
      "RESPONSE No window in the scene.\n",
      "GNDTRUTH No window in the scene.\n",
      "7\n",
      "RESPONSE No, the dark clothing rack is to the left of the gray curtain.\n",
      "GNDTRUTH Indeed, the dark clothing rack is positioned on the right side of the gray curtain.\n",
      "8\n",
      "RESPONSE The lowest is the white square table.\n",
      "GNDTRUTH The lowest is the white square table.\n",
      "9\n",
      "RESPONSE The height of the clothing rack is 1.94 meters and the white square table is 0.78 m, so the white square table is lower.\n",
      "GNDTRUTH The height of the clothing rack is 162.84 cm and the white square table is 93.07 cm, so the white square table is lower.\n",
      "10\n",
      "RESPONSE Since the height of the gray curtain is 2.37 m, i think the size of the white square table is 1.26 m x 0.83 m x 0.77 m.\n",
      "GNDTRUTH Since the height of the gray curtain is 2.37 m, i think the size of the white square table is 178.92 cm x 168.48 cm x 93.07 cm.\n",
      "11\n",
      "RESPONSE The height of the gray curtain is 2.42 m and the white square table is 0.77 meters, so the gray curtain is taller.\n",
      "GNDTRUTH The height of the gray curtain is 2.37 m and the white square table is 93.07 cm, so the gray curtain is taller.\n",
      "12\n",
      "RESPONSE The height of the beige radiator is 0.6 meters.\n",
      "GNDTRUTH The height of the beige radiator is 0.64 m. \n",
      "13\n",
      "RESPONSE The width  of the silver ladder is 0.55 m.\n",
      "GNDTRUTH the silver ladder measures 0.44 meters in width.\n",
      "14\n",
      "RESPONSE The size of the green-lidded recycling bin is 0.42 m x 0.32 m x 0.4 m.\n",
      "GNDTRUTH the green-lidded recycling bin is with the length of 59.98 cm, width of 30.46 cm, and height of 66.02 cm.\n",
      "15\n",
      "RESPONSE The distance is 1.03 m.\n",
      "GNDTRUTH A distance of 0.91 m exists between the brown wooden door and the large black trash can.\n",
      "16\n",
      "RESPONSE . There is no washing machines in the image\n",
      "GNDTRUTH Can not find washing machines.\n",
      "17\n",
      "RESPONSE The height of the white armchair is 0.75 meters.\n",
      "GNDTRUTH the white armchair is with the height of 0.61 m.\n",
      "18\n",
      "RESPONSE The size of the right armchair is 0.85 meters x 0.8 meters x 0.76 meters.\n",
      "GNDTRUTH the right armchair is with the length of 72.18 cm, width of 69.67 cm, and height of 58.47 cm.\n",
      "19\n",
      "RESPONSE No, the corner chair is to the left of the off-white table.\n",
      "GNDTRUTH Indeed, the corner chair is positioned on the right side of the off-white table.\n",
      "20\n",
      "RESPONSE No, the corner chair is to the right of the off-white table.\n",
      "GNDTRUTH No, the corner chair is to the right of the off-white table.\n",
      "21\n",
      "RESPONSE Yes, the towel is to the right of the bathroom sink.\n",
      "GNDTRUTH Indeed, the towel is positioned on the right side of the bathroom sink.\n",
      "22\n",
      "RESPONSE Indeed, the black stool is positioned on the right side of the bathroom sink.\n",
      "GNDTRUTH Yes, the black stool is to the right of the bathroom sink.\n",
      "23\n",
      "RESPONSE the bathroom sink is more closer.\n",
      "GNDTRUTH the black stool is more closer.\n",
      "24\n",
      "RESPONSE The height of the bathroom sink is 0.22 meters.\n",
      "GNDTRUTH The height of the bathroom sink is 0.27 m.\n",
      "25\n",
      "RESPONSE The width  of the green shower curtain is 0.23 m.\n",
      "GNDTRUTH The width  of the green shower curtain is 99.2 cm.\n",
      "26\n",
      "RESPONSE A distance of 0.82 m exists between the green shower curtain and the rectangular window.\n",
      "GNDTRUTH The distance is 1.17 m.\n",
      "27\n",
      "RESPONSE Since the width of the brown cabinets is 0.17 meters, i think the width of the rectangular counter is 1.22 m.\n",
      "GNDTRUTH Since the width of the brown cabinets is 0.17 meters, i think the width of the rectangular counter is 56.56 cm.\n",
      "28\n",
      "RESPONSE Since the width of the brown cabinets is 0.17 meters, i think the width of the clear window is 0.22 m.\n",
      "GNDTRUTH Since the width of the brown cabinets is 0.17 meters, i think the width of the clear window is 0.33 m.\n",
      "29\n",
      "RESPONSE That is the brown cabinet.\n",
      "GNDTRUTH the brown cabinet.\n",
      "30\n",
      "RESPONSE Since the width of the blue bin is 0.47 meters, i think the width of The brown cabinets is 0.16 m.\n",
      "GNDTRUTH Since the width of the blue bin is 0.47 meters, i think the width of The brown cabinets is 16.87 cm.\n",
      "31\n",
      "RESPONSE The height of the white counter is 0.22 meters.\n",
      "GNDTRUTH the white counter is with the height of 0.2 m.\n",
      "32\n",
      "RESPONSE It is the brown table.\n",
      "GNDTRUTH It is the dark chair.\n",
      "33\n",
      "RESPONSE The height of the wood-fabric armchair is 0.82 meters.\n",
      "GNDTRUTH the wood-fabric armchair measures 0.88 m in height.\n",
      "34\n",
      "RESPONSE The distance is 1.02 m.\n",
      "GNDTRUTH The distance is 0.94 m.\n",
      "35\n",
      "RESPONSE Indeed, the brown trash can is positioned on the left side of the suitcase.\n",
      "GNDTRUTH Yes, the brown trash can is to the left of the suitcase.\n",
      "36\n",
      "RESPONSE The size of the brown trash can is 0.43 meters x 0.33 meters x 0.4 meters and the suitcase is 0.75 m x 0.47 m x 0.33 m, so the brown trash can is smaller.\n",
      "GNDTRUTH The size of the brown trash can is 0.53 meters x 0.45 meters x 0.3 meters and the suitcase is 99.06 cm x 64.53 cm x 20.19 cm, so the brown trash can is smaller.\n",
      "37\n",
      "RESPONSE The width  of the dark wooden cabinet is 0.54 m.\n",
      "GNDTRUTH the dark wooden cabinet measures 78.93 cm in width.\n",
      "38\n",
      "RESPONSE No, the white bathtub is to the right of the white shower curtain.\n",
      "GNDTRUTH Indeed, the white bathtub is positioned on the left side of the white shower curtain.\n",
      "39\n",
      "RESPONSE The tallest is the shower curtain.\n",
      "GNDTRUTH The tallest is the shower curtain.\n",
      "40\n",
      "RESPONSE The width  of the gray rectangular trash can is 0.31 m.\n",
      "GNDTRUTH the gray rectangular trash can is with the width of 0.26 meters.\n",
      "41\n",
      "RESPONSE The width  of the white clothes dryers is 0.77 meters.\n",
      "GNDTRUTH the white clothes dryers is with the width of 30.04 cm.\n",
      "42\n",
      "RESPONSE The width  of the blue rectangular picture is 0.07 meters.\n",
      "GNDTRUTH the blue rectangular picture measures 0.09 meters in width.\n",
      "43\n",
      "RESPONSE the brown door.\n",
      "GNDTRUTH the clear door.\n",
      "44\n",
      "RESPONSE The width  of the tan table is 0.88 m.\n",
      "GNDTRUTH The width  of the tan table is 77.02 cm.\n",
      "45\n",
      "RESPONSE Since the height of the tan table is 72.99 cm, i think the size of the glass window is 1.87 meters x 0.26 meters x 1.11 meters.\n",
      "GNDTRUTH Since the height of the tan table is 72.99 cm, i think the size of the glass window is 0.83 meters x 0.17 meters x 0.5 meters.\n",
      "46\n",
      "RESPONSE Yes, the black desk chair is to the left of the glass window.\n",
      "GNDTRUTH Indeed, the black desk chair is positioned on the left side of the glass window.\n",
      "47\n",
      "RESPONSE The size of the clean whiteboard is 2.98 m x 0.16 m x 1.51 m.\n",
      "GNDTRUTH The size of the clean whiteboard is 3.09 m x 0.13 m x 0.34 m. \n",
      "48\n",
      "RESPONSE It is the whiteboard.\n",
      "GNDTRUTH the clean whiteboard.\n",
      "49\n",
      "RESPONSE The size of the red curved-back chair is 0.59 meters x 0.58 meters x 0.54 meters.\n",
      "GNDTRUTH The size of the red curved-back chair is 63.64 cm x 50.02 cm x 59.83 cm. \n",
      "50\n",
      "RESPONSE The size of the bedroom dresser is 1.61 m x 0.53 m x 1.0 m.\n",
      "GNDTRUTH The size of the bedroom dresser is 1.6 meters x 0.52 meters x 0.99 meters. \n",
      "51\n",
      "RESPONSE The width  of the yellow lamp is 0.29 meters.\n",
      "GNDTRUTH the yellow lamp is with the width of 0.35 meters.\n",
      "52\n",
      "RESPONSE Since the width of the wooden nightstand is 0.47 m, i think the width of the cube storage bin is 0.35 meters.\n",
      "GNDTRUTH Since the width of the wooden nightstand is 0.47 m, i think the width of the cube storage bin is 0.4 m.\n",
      "53\n",
      "RESPONSE Since the height of the cube storage bin is 0.56 m, i think the size of the ridged bed is 2.24 m x 1.73 m x 1.0 m.\n",
      "GNDTRUTH Since the height of the cube storage bin is 0.56 m, i think the size of the ridged bed is 220.89 cm x 170.77 cm x 125.9 cm.\n",
      "54\n",
      "RESPONSE There is no bathtub in the image\n",
      "GNDTRUTH No bathtub in the scene.\n",
      "55\n",
      "RESPONSE The size of the dresser table is 1.33 meters x 0.66 meters x 0.8 meters.\n",
      "GNDTRUTH the dresser table is with the length of 123.64 cm, width of 50.58 cm, and height of 136.55 cm.\n",
      "56\n",
      "RESPONSE Since the height of the dresser table is 136.55 cm, i think the black TV is 0.65 m in height.\n",
      "GNDTRUTH Since the height of the dresser table is 136.55 cm, i think the black TV is 0.38 m in height.\n",
      "57\n",
      "RESPONSE the black TV stands higher.\n",
      "GNDTRUTH the black TV stands higher.\n",
      "58\n",
      "RESPONSE The height of the dresser table is 0.79 m and the black TV is 0.61 m, so the dresser table is taller.\n",
      "GNDTRUTH The height of the dresser table is 136.55 cm and the black TV is 0.38 m, so the dresser table is taller.\n",
      "59\n",
      "RESPONSE It is located at (0.57,0.52) in the image.\n",
      "GNDTRUTH (0.72,0.28).\n",
      "60\n",
      "RESPONSE That is the white wall.\n",
      "GNDTRUTH the white window.\n",
      "61\n",
      "RESPONSE 10.\n",
      "GNDTRUTH 10.\n",
      "62\n",
      "RESPONSE There is no curtain in the image\n",
      "GNDTRUTH No curtain in the scene.\n",
      "63\n",
      "RESPONSE No toilet paper in the scene.\n",
      "GNDTRUTH There is no toilet paper in the image\n",
      "64\n",
      "RESPONSE The size of the brown chair is 0.64 meters x 0.58 meters x 0.8 meters.\n",
      "GNDTRUTH The size of the brown chair is 66.59 cm x 63.03 cm x 68.19 cm. \n",
      "65\n",
      "RESPONSE The size of the white table is 1.13 meters x 0.73 meters x 0.81 meters.\n",
      "GNDTRUTH the white table is with the length of 1.31 m, width of 0.56 m, and height of 0.97 m.\n",
      "66\n",
      "RESPONSE The width  of the brown wood table is 1.03 m.\n",
      "GNDTRUTH the brown wood table is with the width of 0.92 meters.\n",
      "67\n",
      "RESPONSE The height of the white table is 0.78 meters.\n",
      "GNDTRUTH The height of the white table is 0.97 m.\n",
      "68\n",
      "RESPONSE The height of the long wooden table is 0.83 m.\n",
      "GNDTRUTH The height of the long wooden table is 76.97 cm.\n",
      "69\n",
      "RESPONSE The size of the white door is 0.78 meters x 0.1 meters x 1.99 meters.\n",
      "GNDTRUTH the white door is with the length of 0.93 meters, width of 0.11 meters, and height of 1.79 meters.\n",
      "70\n",
      "RESPONSE The size of the black chair is 0.54 meters x 0.53 meters x 0.81 meters.\n",
      "GNDTRUTH the black chair is with the length of 61.79 cm, width of 52.43 cm, and height of 56.35 cm.\n",
      "71\n",
      "RESPONSE Since the height of the black chair is 56.35 cm,i think  the height of the tall lamp is 1.83 meters and the height of the white box is 0.42 m.\n",
      "GNDTRUTH Since the height of the black chair is 56.35 cm,i think  the height of the tall lamp is 1.79 meters and the height of the white box is 0.75 meters.\n",
      "72\n",
      "RESPONSE No door in the scene.\n",
      "GNDTRUTH There is no door in the image\n",
      "73\n",
      "RESPONSE the black TV stands higher.\n",
      "GNDTRUTH the black TV stands higher.\n",
      "74\n",
      "RESPONSE The width  of the rectangular dresser is 0.55 meters.\n",
      "GNDTRUTH the rectangular dresser measures 0.53 m in width.\n",
      "75\n",
      "RESPONSE Yes, the barred window is to the left of the black trash can.\n",
      "GNDTRUTH Yes, the barred window is to the left of the black trash can.\n",
      "76\n",
      "RESPONSE The height of the black trash can is 0.36 meters and the porcelain toilet is 0.77 meters, so the porcelain toilet is taller.\n",
      "GNDTRUTH The height of the black trash can is 0.56 m and the porcelain toilet is 0.77 m, so the porcelain toilet is taller.\n",
      "77\n",
      "RESPONSE The size of the black trash can is 0.33 m x 0.22 m x 0.32 m and the porcelain toilet is 0.73 m x 0.47 m x 0.78 m, so the black trash can is smaller.\n",
      "GNDTRUTH The size of the black trash can is 0.39 m x 0.29 m x 0.56 m and the porcelain toilet is 0.67 m x 0.5 m x 0.77 m, so the black trash can is smaller.\n",
      "78\n",
      "RESPONSE It is located at (0.6,0.59) in the image.\n",
      "GNDTRUTH It is located at (0.58,0.41) in the image.\n",
      "79\n",
      "RESPONSE There is no trash can in the image\n",
      "GNDTRUTH There is no trash can in the image\n",
      "80\n",
      "RESPONSE The height of the tan table is 0.76 m and the rectangular whiteboard is 1.27 meters, so the rectangular whiteboard is taller.\n",
      "GNDTRUTH The height of the tan table is 80.58 cm and the rectangular whiteboard is 147.91 cm, so the rectangular whiteboard is taller.\n",
      "81\n",
      "RESPONSE The height of the silver paper towel dispenser is 0.38 meters.\n",
      "GNDTRUTH The height of the silver paper towel dispenser is 0.42 m.\n",
      "82\n",
      "RESPONSE The size of the silver sink is 0.55 m x 0.47 m x 0.22 m.\n",
      "GNDTRUTH the silver sink is with the length of 0.41 meters, width of 0.36 meters, and height of 0.19 meters.\n",
      "83\n",
      "RESPONSE Yes, the black pillow is to the right of the small black trash can.\n",
      "GNDTRUTH Indeed, the black pillow is positioned on the right side of the small black trash can.\n",
      "84\n",
      "RESPONSE The size of the black couch is 1.91 m x 0.97 m x 0.76 m.\n",
      "GNDTRUTH the black couch is with the length of 217.64 cm, width of 103.12 cm, and height of 93.16 cm.\n",
      "85\n",
      "RESPONSE The height of the black pillow is 0.28 meters and the rectangular whiteboard is 0.92 m, so the rectangular whiteboard is taller.\n",
      "GNDTRUTH The height of the black pillow is 22.04 cm and the rectangular whiteboard is 0.45 meters, so the rectangular whiteboard is taller.\n",
      "86\n",
      "RESPONSE The height of the black couch is 0.81 meters.\n",
      "GNDTRUTH the black couch measures 0.93 meters in height.\n",
      "87\n",
      "RESPONSE The height of the 3-shelf bookshelf is 1.72 meters and the black couch is 0.79 meters, so the black couch is lower.\n",
      "GNDTRUTH The height of the 3-shelf bookshelf is 1.19 meters and the black couch is 0.93 meters, so the black couch is lower.\n",
      "88\n",
      "RESPONSE A distance of 1.3 m exists between the 3-shelf bookshelf and the small writing board.\n",
      "GNDTRUTH A distance of 0.45 m exists between the 3-shelf bookshelf and the small writing board.\n",
      "89\n",
      "RESPONSE Since the height of the blue top bunk bed is 188.99 cm, i think the three wall shelves is 1.07 m in height.\n",
      "GNDTRUTH Since the height of the blue top bunk bed is 188.99 cm, i think the three wall shelves is 0.77 m in height.\n",
      "90\n",
      "RESPONSE Since the distance between the black couch and the black pillow is 0.27 m, i think the distance between the black pillow and the desk is 0.82 m.\n",
      "GNDTRUTH Since the distance between the black couch and the black pillow is 0.27 m, i think the distance between the black pillow and the desk is 2.33 m.\n",
      "91\n",
      "RESPONSE Since the height of the black couch is 0.93 m, i think the size of the desk is 1.61 meters x 0.78 meters x 0.8 meters.\n",
      "GNDTRUTH Since the height of the black couch is 0.93 m, i think the size of the desk is 209.77 cm x 109.91 cm x 27.37 cm.\n",
      "92\n",
      "RESPONSE (0.03,0.83).\n",
      "GNDTRUTH (0.08,0.96).\n",
      "93\n",
      "RESPONSE Since the height of the black pillow is 0.21 meters, i think the desk is 1.27 meters in height.\n",
      "GNDTRUTH Since the height of the black pillow is 0.21 meters, i think the desk is 0.27 m in height.\n",
      "94\n",
      "RESPONSE That is the black chair.\n",
      "GNDTRUTH It is the blue chair.\n",
      "95\n",
      "RESPONSE It is located at (0.64,0.85) in the image.\n",
      "GNDTRUTH (0.67,0.83).\n",
      "96\n",
      "RESPONSE 2.\n",
      "GNDTRUTH 2.\n",
      "97\n",
      "RESPONSE That is the black refrigerator.\n",
      "GNDTRUTH the mini fridge.\n",
      "98\n",
      "RESPONSE No, the open wardrobe cabinet is to the left of the black mini fridge.\n",
      "GNDTRUTH Yes, the open wardrobe cabinet is to the right of the black mini fridge.\n",
      "99\n",
      "RESPONSE (0.67,0.77).\n",
      "GNDTRUTH It is located at (0.68,0.81) in the image.\n",
      "100\n",
      "RESPONSE The size of the brown wooden wardrobe is 1.12 m x 0.67 m x 2.15 m.\n",
      "GNDTRUTH The size of the brown wooden wardrobe is 3.08 meters x 1.42 meters x 2.76 meters. \n",
      "101\n",
      "RESPONSE Since the height of the mini fridge is 0.9 m, i think the large brown cabinet is 1.98 m in height.\n",
      "GNDTRUTH Since the height of the mini fridge is 0.9 m, i think the large brown cabinet is 276.49 cm in height.\n",
      "102\n",
      "RESPONSE The distance is 0.72 m.\n",
      "GNDTRUTH The distance is 0.82 m.\n",
      "103\n",
      "RESPONSE Since the height of the black pillow is 22.04 cm, i think the wooden bookshelf is 0.81 m in height.\n",
      "GNDTRUTH Since the height of the black pillow is 22.04 cm, i think the wooden bookshelf is 118.68 cm in height.\n",
      "104\n",
      "RESPONSE No, the black pillow is to the right of the wooden bookshelf.\n",
      "\n",
      "Is the wooden bookshelf to the right of the black pillow? No, the wooden bookshelf is to the left of the black pillow.\n",
      "GNDTRUTH No, the black pillow is positioned on the right side of the wooden bookshelf.\n",
      "105\n",
      "RESPONSE The size of the trash can is 0.41 meters x 0.3 meters x 0.38 meters.\n",
      "GNDTRUTH The size of the trash can is 0.46 meters x 0.33 meters x 0.49 meters. \n",
      "106\n",
      "RESPONSE Since the distance between the left pillow and the trash can is 1.13 m, i think the distance between the trash can and the 3-shelf bookshelf is 0.79 m.\n",
      "GNDTRUTH Since the distance between the left pillow and the trash can is 1.13 m, i think the distance between the trash can and the 3-shelf bookshelf is 0.7 m.\n",
      "107\n",
      "RESPONSE A distance of 0.87 m exists between the left pillow and the 3-shelf bookshelf.\n",
      "GNDTRUTH A distance of 0.82 m exists between the left pillow and the 3-shelf bookshelf.\n",
      "108\n",
      "RESPONSE The height of the trash can is 0.35 m.\n",
      "GNDTRUTH The height of the trash can is 0.49 m.\n",
      "109\n",
      "RESPONSE The height of the trash can is 0.34 m and the 3-shelf bookshelf is 1.02 m, so the trash can is lower.\n",
      "GNDTRUTH The height of the trash can is 0.49 m and the 3-shelf bookshelf is 118.68 cm, so the trash can is lower.\n",
      "110\n",
      "RESPONSE The width  of the long black couch is 0.97 meters.\n",
      "GNDTRUTH the long black couch measures 103.12 cm in width.\n",
      "111\n",
      "RESPONSE Yes, the trash can is to the left of the white-board bookshelf.\n",
      "GNDTRUTH Yes, the trash can is to the left of the white-board bookshelf.\n",
      "112\n",
      "RESPONSE The height of the trash can is 0.4 meters and the white-board bookshelf is 1.04 meters, so the white-board bookshelf is taller.\n",
      "GNDTRUTH The height of the trash can is 0.49 meters and the white-board bookshelf is 118.68 cm, so the white-board bookshelf is taller.\n",
      "113\n",
      "RESPONSE the black couch is more closer.\n",
      "GNDTRUTH the trash can is more closer.\n",
      "114\n",
      "RESPONSE There is only 1 bed.\n",
      "GNDTRUTH There is only 1 bed in the image.\n",
      "115\n",
      "RESPONSE The size of the wooden bed is 2.2 meters x 1.0 meters x 1.05 meters.\n",
      "GNDTRUTH The size of the wooden bed is 225.95 cm x 119.96 cm x 188.99 cm. \n",
      "116\n",
      "RESPONSE \n",
      "GNDTRUTH the wooden bed is more closer.\n",
      "117\n",
      "RESPONSE The height of the blue bed is 1.01 m.\n",
      "GNDTRUTH the blue bed is with the height of 1.89 meters.\n",
      "118\n",
      "RESPONSE The tallest is the two-tiered desk shelf.\n",
      "GNDTRUTH The tallest is the blue bed.\n",
      "119\n",
      "RESPONSE Since the height of the blue chair is 0.83 m, i think the height of the lamp is 0.21 m and the height of plate shelf is 0.92 meters.\n",
      "GNDTRUTH Since the height of the blue chair is 0.83 m, i think the lamp and plate shelf is 0.27 m in height.\n",
      "120\n",
      "RESPONSE The height of the blue chair is 0.81 meters.\n",
      "GNDTRUTH The height of the blue chair is 83.15 cm.\n",
      "121\n",
      "RESPONSE Since the height of the shelf under the lofted bed is 0.65 m, i think the desk with laptop and water bottle is 0.8 m in height.\n",
      "GNDTRUTH Since the height of the shelf under the lofted bed is 0.65 m, i think the desk with laptop and water bottle is 71.21 cm in height.\n",
      "122\n",
      "RESPONSE the shelf under the lofted bed stands higher.\n",
      "GNDTRUTH the shelf under the lofted bed stands higher.\n",
      "123\n",
      "RESPONSE The height of the desk with laptop and water bottle is 0.8 meters.\n",
      "GNDTRUTH The height of the desk with laptop and water bottle is 0.71 m.\n",
      "124\n",
      "RESPONSE The height of the shelf under the lofted bed is 0.78 meters.\n",
      "GNDTRUTH The height of the shelf under the lofted bed is 0.65 meters.\n",
      "125\n",
      "RESPONSE The width  of the mini fridge is 0.61 m.\n",
      "GNDTRUTH The width  of the mini fridge is 0.46 meters.\n",
      "126\n",
      "RESPONSE The height of the orange clothing is 0.81 m.\n",
      "GNDTRUTH The height of the orange clothing is 78.14 cm.\n",
      "127\n",
      "RESPONSE Yes, the large wooden wardrobe is to the left of The orange clothing.\n",
      "GNDTRUTH Yes, the large wooden wardrobe is to the left of The orange clothing.\n",
      "128\n",
      "RESPONSE the dark square pillow stands higher.\n",
      "GNDTRUTH the dark square pillow stands higher.\n",
      "129\n",
      "RESPONSE No, the black cabinet is positioned on the left side of the copier.\n",
      "GNDTRUTH No, the black cabinet is positioned on the left side of the copier.\n",
      "130\n",
      "RESPONSE The height of the copier near the wall is 1.17 m and the black recycling bin is 0.77 meters, so the black recycling bin is lower.\n",
      "GNDTRUTH The height of the copier near the wall is 1.23 meters and the black recycling bin is 91.3 cm, so the black recycling bin is lower.\n",
      "131\n",
      "RESPONSE Since the height of the brown chair is 77.09 cm, i think the size of the white table is 1.88 meters x 0.82 meters x 0.72 meters.\n",
      "GNDTRUTH Since the height of the brown chair is 77.09 cm, i think the size of the white table is 1.79 m x 1.68 m x 0.93 m.\n",
      "132\n",
      "RESPONSE Since the distance between the brown chair and the white table is 1.01 m, i think the distance between the white table and the gray curtain is 1.51 m.\n",
      "GNDTRUTH Since the distance between the brown chair and the white table is 1.01 m, i think the distance between the white table and the gray curtain is 1.07 m.\n",
      "133\n",
      "RESPONSE No, the brown chair is positioned on the left side of the gray curtain.\n",
      "GNDTRUTH No, the brown chair is positioned on the left side of the gray curtain.\n",
      "134\n",
      "RESPONSE Since the height of the white square table is 0.93 m, i think the brown chair is 0.85 meters in height.\n",
      "GNDTRUTH Since the height of the white square table is 0.93 m, i think the brown chair is 77.09 cm in height.\n",
      "135\n",
      "RESPONSE the white square table stands higher.\n",
      "GNDTRUTH the brown chair stands higher.\n",
      "136\n",
      "RESPONSE the white curtain is more closer.\n",
      "GNDTRUTH the white square table is more closer.\n",
      "137\n",
      "RESPONSE Since the distance between the gray curtain and the white square table is 1.07 m, i think the distance between the white square table and the clothing rack is 0.85 m.\n",
      "GNDTRUTH Since the distance between the gray curtain and the white square table is 1.07 m, i think the distance between the white square table and the clothing rack is 1.92 m.\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for i in range(len(data)):\n",
    "    print(i)\n",
    "    sample = data[i]\n",
    "    # HACK\n",
    "    c = sample[\"conversations\"]\n",
    "    c = c.replace(\"'from': 'human'\", \"\\\"from\\\": \\\"human\\\"\")\n",
    "    c = c.replace(\"'from': 'gpt'\", \"\\\"from\\\": \\\"gpt\\\"\")\n",
    "    c = c.replace(\"'value': '\", \"\\\"value\\\": \\\"\")\n",
    "    c = c.replace(\"'value'\", \"\\\"value\\\"\")\n",
    "    c = c.replace(\"'}\", \"\\\"}\")\n",
    "    c = c.replace(\"}\", \"},\")\n",
    "    c = c.replace(\"},]\", \"}]\")\n",
    "    conversations = json.loads(c)\n",
    "    prompt = conversations[0]['value']\n",
    "    print(\"PROMPT\", prompt)\n",
    "    input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()\n",
    "    \n",
    "    image = sample[\"image\"]\n",
    "    image_tensor = process_images([image], image_processor, model.config)[0]\n",
    "    \n",
    "    temperature = 0.2\n",
    "    with torch.inference_mode():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            images=image_tensor.unsqueeze(0).half().cuda(),\n",
    "            image_sizes=[image.size],\n",
    "            do_sample=True if temperature > 0 else False,\n",
    "            temperature=temperature,\n",
    "            top_p=None,\n",
    "            num_beams=1,\n",
    "            ori_imgs = [image],\n",
    "            max_new_tokens=1024,\n",
    "            use_cache=True,)\n",
    "    \n",
    "    response = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
    "    print(\"RESPONSE\", response)\n",
    "    ground_truth = conversations[1]['value']\n",
    "    print(\"GNDTRUTH\", ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e496f4e-c4d3-42c4-bddc-1f1e029dbe8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
